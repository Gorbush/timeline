# -----
# This is for usage with docker, which is the preferred setzp
# Adjust and rename/copy it to .env
# -----
# This is where to look for your photos and videos, nothing will be changed or modified here
ASSET_PATH=/Users/johndoe/photos

# Where to upload
# will be a folder below the ASSET_PATH
UPLOAD_FOLDER=uploaded

# Here are previews created for providing downscaled version to be delivered by nginx
PREVIEW_PATH=/Users/johndoe/timeline/data/preview

# Where to put Log-Files
LOG_PATH=/Users/johndoe/timeline/timeline/data/log

# Where to put Files from mariaDB
DATABASE_DATA=/Users/johndoe/timeline/data/db

# Same for rabbitmq
RABBITMQ_DATA=/Users/johndoe/timeline/data/rabbitmq

# Adjust this with the number of parallel workers / tasks to be allowed
# Depends on your machine; for a NAS with 4GB and 4 processors a limit of 1
# seems reasonable
# On something more powerful 4-6 is good
# In general a good approach is to go with the number of physical processors
# The workers are on one hand CPU intensive but on the other hand also
# memory intensive as the loaded models for face and thing detection
# already eat up a a few hundret megabyte. 
# That means that a single worker process need around 1.9GB RAM. 
# In case you only have as NAS with 4GB just just use one. With 8GB 2-3 should work.
# On my Dev machine with 16GB RAM 6 workers are ok
# Once the workers are done anymore they will shut down and only 1 worker will 
# remain. That means if the initial work is done the memory (and CPU) 
# footprint should reduce  
WORKERS_PROCESS=2

# vvv --- Ignore this for now. Old
# There are two workers, one that just reads the photos, extracts exif, 
# creates preview and so on. They are CPU intensive but not very memory intensive 
# When everything is done, those worker processes will be scaled down automatically
# by Celery
# The other worker does all the machine learning stuff. Due to the loaded models
# they are CPU intensive and quite memory intensive, so we create fewer of them
# on process can eat up to 2.5GB Memory. So be reasonable with them
# Unfortunatley Tensorflow/Keras seems not to be thread safe, so we have to used
# prefork workers. Moreoever, each forked process need very own instances 
# of each model. That is why they are so memory intensive. 
# Also the autoscaling from Celery Unfortunatley does not work here
# WORKERS_ANALYZE=2
# ^^^ -- not used atm

# This is the password to access adminer
# under http://<machine>:9091/adminer
# with the root user
DB_SUPER_USER_PW=example

# This controls the Flask Webapp and the number and type of workers
# You might want to play with the number of workers
# According to the gunicorn documentation this should be between 1 and 2 per core
# but most if the load is handled by nginx, so this shouldn't be necessary
GUNICORN_WORKERS=4
# This is the default, other options like eventlet can also be used
GUNICORN_WORKER_CLASS=sync

# Face detection tweaks
# This values determines the range of which faces are consided to be the Same
# during the clustering. A greater value results in a more 
# "generous" face clustering
FACE_CLUSTER_EPSILON=0.6
# This determines how many similar faces are required to form a cluster
FACE_CLUSTER_MIN_SAMPLES=5
# How many faces are considered for the clustering
# The more, the better but it has an effect and performance and memory consumption
FACE_CLUSTER_MAX_FACES=8000

# This determines the boundaries when a face is considered to be identical
# Smaller values result more false negative matches => Faces are not named at all
# Bigger values result in more false positive matches => Faces are are named wrong
FACE_DISTANCE_VERY_SAFE=0.55
FACE_DISTANCE_SAFE=0.65
FACE_DISTANCE_MAYBE=0.75
